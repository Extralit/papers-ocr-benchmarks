{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b485ee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz    \n",
    "import ocrmypdf\n",
    "from pathlib import Path\n",
    "import pymupdf4llm \n",
    "import logging\n",
    "import re\n",
    "import weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "57482ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging for error tracking and debugging\n",
    "logging.basicConfig(\n",
    "    filename=\"extract_pipeline.log\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c15993e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directories if not exist\n",
    "Path(\"temp_ocr\").mkdir(exist_ok=True)\n",
    "Path(\"output_chunk\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "824f5c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LITSERVE_OCR_URL = \"http://localhost:8001/ocr\"  # we will use the prod url here\n",
    "WEAVIATE_URL = \"http://localhost:8080\"  # same here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ab4b27f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pdf(input_path, ocr_path, use_litserve=False, litserve_url=None):\n",
    "    \"\"\"\n",
    "    Run OCRmyPDF (locally or via endpoint) only if the PDF is scanned.\n",
    "    Returns the path to a PDF with selectable text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(str(input_path))\n",
    "        is_scanned = all(not page.get_text().strip() for page in doc)\n",
    "        if is_scanned:\n",
    "            logging.info(f\"ðŸ§¾ Detected scanned PDF â†’ running OCR on {input_path}\")\n",
    "            # Optionally use LitServe for OCR endpoint\n",
    "            if use_litserve and litserve_url:\n",
    "                import requests\n",
    "                with open(input_path, \"rb\") as f:\n",
    "                    response = requests.post(litserve_url, files={\"file\": f})\n",
    "                if response.status_code != 200:\n",
    "                    raise RuntimeError(\"LitServe OCR failed\")\n",
    "                with open(ocr_path, \"wb\") as fout:\n",
    "                    fout.write(response.content)\n",
    "            else:\n",
    "                ocrmypdf.ocr(\n",
    "                    input_file=str(input_path),\n",
    "                    output_file=str(ocr_path),\n",
    "                    rotate_pages=True,\n",
    "                    deskew=True,\n",
    "                    force_ocr=True,\n",
    "                    skip_text=True,\n",
    "                    progress_bar=False\n",
    "                )\n",
    "            return ocr_path\n",
    "        else:\n",
    "            logging.info(f\"ðŸ“„ Bornâ€‘digital PDF â†’ skipping OCR for {input_path}\")\n",
    "            return Path(input_path)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during OCR preprocessing: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2d054fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tables_from_markdown(md_text):\n",
    "    \"\"\"\n",
    "    Remove markdown tables (pipe `|` blocks) from extracted markdown.\n",
    "    \"\"\"\n",
    "    # Matches markdown tables: lines with pipes and then at least one header separator row\n",
    "    pattern = r'(?:^.*?\\|.*?\\n)(?:^[ \\t]*\\|?[-:]+[\\| -:]+[\\n\\r])(?:^.*?\\|.*?\\n?)+'\n",
    "    cleaned = re.sub(pattern, '', md_text, flags=re.MULTILINE)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3b30f85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_section_headers(md_text, min_length=20):\n",
    "    \"\"\"\n",
    "    Chunk markdown by section headers, preserving header hierarchy for TOC-aware splits.\n",
    "    Returns list of dicts: 'header', 'text', 'start_line', 'end_line'\n",
    "    \"\"\"\n",
    "    lines = md_text.splitlines()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_header = None\n",
    "    start_line = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        header_match = re.match(r\"^(#+)\\s+(.+)\", line)\n",
    "        if header_match:\n",
    "            if current_chunk:\n",
    "                text = \"\\n\".join(current_chunk).strip()\n",
    "                if len(text) >= min_length:\n",
    "                    chunks.append({\n",
    "                        \"header\": current_header,\n",
    "                        \"text\": text,\n",
    "                        \"start_line\": start_line,\n",
    "                        \"end_line\": i-1,\n",
    "                    })\n",
    "            current_chunk = [line]\n",
    "            current_header = header_match.group(2)\n",
    "            start_line = i\n",
    "        else:\n",
    "            current_chunk.append(line)\n",
    "    if current_chunk:\n",
    "        text = \"\\n\".join(current_chunk).strip()\n",
    "        if len(text) >= min_length:\n",
    "            chunks.append({\n",
    "                \"header\": current_header,\n",
    "                \"text\": text,\n",
    "                \"start_line\": start_line,\n",
    "                \"end_line\": len(lines)-1,\n",
    "            })\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "533b667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_chunk(pdf_path, output_dir, chunk_mode='header', chunk_size=1024, chunk_overlap=128):\n",
    "    \"\"\"\n",
    "    Extracts markdown (hierarchical, TOC-aware) from PDF, removes tables,\n",
    "    chunks using either LlamaMarkdownReader (token-sized) or manual header splitting,\n",
    "    and writes markdown chunks for inspection.\n",
    "    Returns: list of LlamaIndexDoc-like dicts, each with .text and .metadata\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Extracting markdown from {pdf_path}\")\n",
    "\n",
    "        # Extract markdown with TOC/hierarchy\n",
    "        # NOTE: Set extract_tables=False if your pymupdf4llm version supports it.\n",
    "        try:\n",
    "            md_text = pymupdf4llm.to_markdown(\n",
    "                str(pdf_path),\n",
    "                include_toc=True,\n",
    "                write_images=False,\n",
    "                # extract_tables=False\n",
    "            )\n",
    "        except TypeError:\n",
    "            # For older versions w/o extract_tables param\n",
    "            md_text = pymupdf4llm.to_markdown(\n",
    "                str(pdf_path),\n",
    "                include_toc=True,\n",
    "                write_images=False\n",
    "            )\n",
    "        # Remove tables regardless (extra safety)\n",
    "        md_text = remove_tables_from_markdown(md_text)\n",
    "        # Optional: Write intermediate\n",
    "        debug_md = Path(output_dir) / f\"{Path(pdf_path).stem}_ALL.md\"\n",
    "        debug_md.write_text(md_text, encoding=\"utf-8\")\n",
    "\n",
    "        # Choose chunking mode:\n",
    "        if chunk_mode == 'llama_token':\n",
    "            # Use LlamaMarkdownReader's own size-based chunking with extra_info\n",
    "            reader = pymupdf4llm.LlamaMarkdownReader(\n",
    "                margins=(0, 50, 0, 30),\n",
    "                max_levels=3,\n",
    "                body_limit=11,\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap\n",
    "            )\n",
    "            docs = reader.load_data(pdf_path)\n",
    "            chunks = [\n",
    "                {\n",
    "                    \"text\": remove_tables_from_markdown(doc.text),\n",
    "                    \"metadata\": doc.extra_info\n",
    "                } for doc in docs\n",
    "            ]\n",
    "        else:\n",
    "            # Manual, semantically cleaner header-based chunking\n",
    "            section_chunks = chunk_by_section_headers(md_text, min_length=30)\n",
    "            chunks = []\n",
    "            for i, chunk in enumerate(section_chunks):\n",
    "                out_file = Path(output_dir) / f\"{Path(pdf_path).stem}_section_{i}.md\"\n",
    "                out_file.write_text(chunk[\"text\"], encoding=\"utf-8\")\n",
    "                # Metadata includes header, line range, source\n",
    "                chunks.append({\n",
    "                    \"text\": chunk[\"text\"],\n",
    "                    \"metadata\": {\n",
    "                        \"header\": chunk[\"header\"],\n",
    "                        \"source_pdf\": str(pdf_path),\n",
    "                        \"chunk_index\": i,\n",
    "                        \"start_line\": chunk[\"start_line\"],\n",
    "                        \"end_line\": chunk[\"end_line\"],\n",
    "                    }\n",
    "                })\n",
    "        logging.info(f\"Produced {len(chunks)} chunks from {pdf_path}\")\n",
    "        return chunks\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Extraction/chunking error: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4d27f52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_chunks_to_weaviate(chunks, weaviate_url=WEAVIATE_URL, class_name=\"SectionChunk\"):\n",
    "    \"\"\"\n",
    "    Send chunks to Weaviate vector DB (example; comment/uncomment as needed).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import weaviate\n",
    "        client = weaviate.Client(weaviate_url)\n",
    "        for chunk in chunks:\n",
    "            # Set properties as per your class schema in Weaviate\n",
    "            obj = {\n",
    "                \"text\": chunk[\"text\"],\n",
    "                \"header\": chunk[\"metadata\"].get(\"header\", \"\"),\n",
    "                \"source_pdf\": chunk[\"metadata\"].get(\"source_pdf\", \"\"),\n",
    "                \"chunk_index\": chunk[\"metadata\"].get(\"chunk_index\", -1),\n",
    "            }\n",
    "            client.batch.add_data_object(obj, class_name)\n",
    "        client.batch.flush()\n",
    "        logging.info(f\"Sent {len(chunks)} to Weaviate at {weaviate_url}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Weaviate upload error: {e}\")\n",
    "        print(f\"Failed to upload to Weaviate: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3edfcad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_pipeline(\n",
    "        pdf_path,\n",
    "        chunk_mode='header',          # 'header' for semantic, 'llama_token' for token-based\n",
    "        use_litserve=False,\n",
    "        litserve_url=None,\n",
    "        send_to_weaviate=False,\n",
    "        weaviate_url=WEAVIATE_URL\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Complete pipeline: PDF â†’ OCR/preprocess â†’ Extract markdown (no tables) â†’ Chunk â†’ (optional: Weaviate upload)\n",
    "    \"\"\"\n",
    "    filename = Path(pdf_path).stem\n",
    "    temp_ocr = Path(\"temp_ocr\"); temp_ocr.mkdir(exist_ok=True)\n",
    "    out_md   = Path(\"output_markdown\"); out_md.mkdir(exist_ok=True)\n",
    "    ocr_pdf = temp_ocr / f\"{filename}_ocr.pdf\"\n",
    "\n",
    "    try:\n",
    "        # Step 1: Preprocess (OCR if necessary)\n",
    "        used_pdf = preprocess_pdf(pdf_path, ocr_pdf, use_litserve=use_litserve, litserve_url=litserve_url)\n",
    "\n",
    "        # Step 2: Extract and chunk\n",
    "        chunks = extract_and_chunk(used_pdf, out_md, chunk_mode=chunk_mode)\n",
    "\n",
    "        print(f\"Extracted {len(chunks)} chunks from {pdf_path}\")\n",
    "        logging.info(f\"Chunks extracted from {pdf_path}\")\n",
    "\n",
    "        # Step 3: Send to Weaviate (if enabled)\n",
    "        if send_to_weaviate and len(chunks) > 0:\n",
    "            send_chunks_to_weaviate(chunks, weaviate_url=weaviate_url)\n",
    "            print(f\"Uploaded {len(chunks)} chunks to Weaviate at {weaviate_url}\")\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Pipeline failed for {pdf_path}: {e}\")\n",
    "        print(f\"Pipeline failed: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ab03093f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline failed: to_markdown() got an unexpected keyword argument 'include_toc'\n",
      "\n",
      "Extracted 0 LlamaIndexDoc-like chunks.\n"
     ]
    }
   ],
   "source": [
    "test_pdf = \"pdfs/Allossogbe_et_al_2017_Mal_J.pdf\"\n",
    "# Choose chunk_mode='header' for section-aligned, or 'llama_token' for token-based chunking\n",
    "chunks = process_pdf_pipeline(\n",
    "    test_pdf,\n",
    "    chunk_mode='header',            # Or 'llama_token' for alternate strategy\n",
    "    use_litserve=False,             # Set True/use URL if using LitServe OCR endpoint\n",
    "    litserve_url=None,              # Set to endpoint address if needed\n",
    "    send_to_weaviate=False,         # Set True if Weaviate upload desired\n",
    "    weaviate_url=\"http://localhost:8080\"\n",
    ")\n",
    "print(f\"\\nExtracted {len(chunks)} LlamaIndexDoc-like chunks.\")\n",
    "if chunks:\n",
    "    print(\"--- Example Chunk ---\")\n",
    "    print(\"Header:\", chunks[0]['metadata'].get('header'))\n",
    "    print(\"Text:\", chunks[0]['text'][:400], \"\\n[...]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
